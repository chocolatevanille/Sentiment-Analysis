{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8dac56-6406-4bfa-8d2b-58f8f7d1c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network module\n",
    "import torch.optim as optim # optimizer module\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler # utilities\n",
    "import torchvision.transforms as transforms # torchvision (computer vision)\n",
    "import torchaudio # torchaudio (audio processing)\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc5e3b4-fe6c-4d8f-b4e4-faa8fe274f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: IMPORT AND PREPROCESS DATA\n",
    "\n",
    "#training data\n",
    "train_df_unfiltered = pd.read_json('Movies_and_TV.json', lines=True)\n",
    "train_df = train_df_unfiltered.loc[:, [\"overall\", \"reviewText\"]]\n",
    "#print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563f77c0-658a-4125-aded-7e3cf1a89237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText\n",
      "0        4  sorry didnt purchase years ago first came good...\n",
      "1        4  believe tell receive blessing watching video c...\n",
      "2        4  seen x live many times early days recent reuni...\n",
      "3        4  excited finally live concert video x ive seen ...\n",
      "4        4  x one best punk bands ever dont even like call...\n"
     ]
    }
   ],
   "source": [
    "# preprocessing functions\n",
    "# cleans up text in reviews\n",
    "# str -> str\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A) # remove numbers and special characters\n",
    "    text = text.lower()\n",
    "    tokens = tokenize(text)\n",
    "    stop_words = set(stopwords.words('english')) # get and remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def shift_score(score):\n",
    "    return (int(score) - 1)\n",
    "    \n",
    "train_df['reviewText'] = train_df['reviewText'].apply(preprocess_text)\n",
    "train_df['overall'] = train_df['overall'].apply(shift_score)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f341b2a-1dd1-4c55-8f59-09d3e2bba700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: TOKENIZE DATA FOR BERT\n",
    "\n",
    "# initialize the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenize the text\n",
    "max_len = 512\n",
    "# tokenizer on every reviewText, special tokens to indicate CLS, SEP, etc. for bert\n",
    "train_df['input_ids'] = train_df['reviewText'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)) \n",
    "# post truncates sequences longer than and pads sequences shorter than max_len from the end \n",
    "input_ids_padded = pad_sequences(train_df['input_ids'].tolist(), maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "train_df['input_ids'] = input_ids_padded.tolist()\n",
    "train_df['attention_masks'] = train_df['input_ids'].apply(lambda seq: [float(i > 0) for i in seq]) # set padding tokens to 0\n",
    "#print(train_df[['overall', 'input_ids', 'attention_masks']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b7ad5c-e2da-4a1f-abd7-d308837ff6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to PyTorch-specific tensors\n",
    "input_ids = torch.tensor(train_df['input_ids'].values.tolist())\n",
    "attention_masks = torch.tensor(train_df['attention_masks'].values.tolist())\n",
    "labels = torch.tensor(train_df['overall'].values)\n",
    "\n",
    "# create the DataLoader for training and validation sets\n",
    "batch_size = 16\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset)) # change this to make more or less of set for training/validation\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20695c7-8b5d-4096-83a1-165584fdcb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using CUDA.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: INITIALIZE MODEL (BERT)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=5,  # number of unique sentiment classes\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"You are using CUDA.\")\n",
    "else:\n",
    "    print(\"You are using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd14ef0-f21d-4e95-b6fa-410aee708593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Training loss: 0.9718153426647186\n",
      "Validation Accuracy: 0.69\n",
      "Epoch 2/4\n",
      "Training loss: 0.784626232624054\n",
      "Validation Accuracy: 0.704\n",
      "Epoch 3/4\n",
      "Training loss: 0.6514057156443596\n",
      "Validation Accuracy: 0.708\n",
      "Epoch 4/4\n",
      "Training loss: 0.5427697497010231\n",
      "Validation Accuracy: 0.707\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: TRAIN THE MODEL\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# optimize parameters\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "epochs = 4 # number of passes through the entire training dataset\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# scheduler initialization\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# training function\n",
    "# NULL -> NULL\n",
    "def train():\n",
    "    model.train() # training mode (enables gradients and dropout)\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        model.zero_grad() # clears gradient from previous batch\n",
    "        \n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels) # forward pass through model\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item() # compute total loss\n",
    "        \n",
    "        loss.backward() # backwards propagation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clips gradient to prevent exploding\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Training loss: {avg_train_loss}\")\n",
    "\n",
    "# validating function\n",
    "# NULL -> NULL\n",
    "def validate():\n",
    "    model.eval() #evaluation mode (disables gradients and dropout)\n",
    "    preds, true_labels = [], []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # without computing gradients, perform forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "\n",
    "        # get model predictions and true labels\n",
    "        logits = outputs.logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy()) \n",
    "        true_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    print(f\"Validation Accuracy: {acc}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7580f9-b222-4708-be2c-134098f01ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sentiment_model\\\\tokenizer_config.json',\n",
       " 'sentiment_model\\\\special_tokens_map.json',\n",
       " 'sentiment_model\\\\vocab.txt',\n",
       " 'sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('sentiment_model')\n",
    "tokenizer.save_pretrained('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "944b3308-b73a-4b2e-8ac6-929fc709f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     comment  sentiment\n",
      "1  worst video ive ever seen          0\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
